{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will work on K-Means clustering. You will be asked to cluster the MNIST dataset using the K-Means algorithm.\n",
    "\n",
    "MNIST is a database of handwritten digits. Each image is 28x28, grayscale, and represents a digit between 0 and 9.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "# set matplotlib to display all plots inline with the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions \n",
    "These functions will help us load and visualize the data. You can simply run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_digit(digit, label=None, training_or_inferred=\"training\", ax=None):\n",
    "    \"\"\"Graphically display a 784x1 vector, representing a digit, and optionally show the corresponding label.\"\"\"\n",
    "    if ax is None:\n",
    "        plt.figure()\n",
    "        fig = plt.imshow(digit.reshape(28,28))\n",
    "    else:\n",
    "        fig = ax.imshow(digit.reshape(28,28))\n",
    "    fig.set_cmap('gray_r')\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)\n",
    "    if label is not None:\n",
    "        if training_or_inferred == \"training\":\n",
    "            title_str = \"GT label: \"\n",
    "        else:\n",
    "            title_str = \"Inferred label: \"\n",
    "        if ax is None:\n",
    "            plt.title(title_str  + str(int(label)))\n",
    "        else:\n",
    "            ax.set_title(title_str  + str(int(label)))\n",
    "\n",
    "def display_centers(centers, title=\"Cluster centers\"):\n",
    "    K = len(centers)\n",
    "    n_row = int(np.ceil(K / 5))\n",
    "\n",
    "    fig, axes = plt.subplots(n_row, 5, figsize=(15, 2.5 * n_row))\n",
    "    fig.suptitle(title)\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < K:\n",
    "            display_digit(centers[i], ax=ax)\n",
    "        else:\n",
    "            ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load and visualize the MNIST dataset\n",
    "\n",
    "The MNIST database contains a total of 70,000 handwritten digits with 10 different classes, from 0 to 9. 60,000 examples are taken as training dataset and the remaining 10,000 as test set. The digits have been size-normalized and centered in a fixed-size image. \n",
    "\n",
    "First download MNIST from: [https://www.python-course.eu/data/mnist/mnist_train.csv](https://www.python-course.eu/data/mnist/mnist_train.csv) and put this csv file in the same folder as your jupyter notebook file. (You may have already done this in a previous exercise, it is fine to reuse the same csv file.)\n",
    "\n",
    "Each image is flattened to a vector of size $784$ with integers in the range $[0, 255]$, and the images can be reshaped to $28 \\times 28$. The value of the pixels indicate the brightness of a pixel, that is, the higher number the brighter the pixel.\n",
    "\n",
    "To make sure everything went fine, we will first load the dataset and visualize some samples from it. It might take a while to load it into memory! We will then subsample the dataset. The MNIST database is huge, and our algorithm will take very long if we want to use all the data. Therefore we will only use the first 1200 samples as our training dataset. We will also use 500 samples for validation and another 500 for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data=(1200, 784), shape of label=(1200,)\n",
      "Shape of validation data=(500, 784), shape of label=(500,)\n",
      "Shape of test data=(500, 784), shape of label=(500,)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "image_size = 28  # width and length\n",
    "n_classes = 10  # i.e. 0, 1, 2, 3, ..., 9\n",
    "image_pixels = image_size * image_size  # 28*28 = 784\n",
    "\n",
    "# Load the data. This loads a Nx785 array, where the first column is actually the label.\n",
    "full_data = np.loadtxt(\"mnist_train.csv\", delimiter=\",\")\n",
    "\n",
    "# Build the train set\n",
    "n_train = 1200\n",
    "n_val = 500\n",
    "n_test = 500\n",
    "\n",
    "x_train = full_data[:n_train, 1:]\n",
    "y_train = full_data[:n_train, 0].astype(int)\n",
    "x_val = full_data[n_train : n_train + n_val, 1:]\n",
    "y_val = full_data[n_train : n_train + n_val, 0].astype(int)\n",
    "x_test = full_data[n_train + n_val : n_train + n_val + n_test, 1:]\n",
    "y_test = full_data[n_train + n_val : n_train + n_val + n_test, 0].astype(int)\n",
    "\n",
    "print(f\"Shape of training data={x_train.shape}, shape of label={y_train.shape}\")\n",
    "print(f\"Shape of validation data={x_val.shape}, shape of label={y_val.shape}\")\n",
    "print(f\"Shape of test data={x_test.shape}, shape of label={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image visualization\n",
    "\n",
    "We can use the helper function `display_digit()` to display the image of each data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We visualize the first 10 samples from the training set, with their label.\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15,5))\n",
    "fig.suptitle(\"Visualizing sample images from training set\")\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    display_digit(x_train[i], y_train[i], \"training\", ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. K-Means clustering\n",
    "\n",
    "In this section we will learn how to implement the K-Means algorithm. The procedure is quite simple:\n",
    "\n",
    "1. Initialize the cluster centers (randomly in our case, but there are more clever methods for initialization out there!)\n",
    "2. Loop until the cluster centers don't move anymore or we reach the max number of iterations:\n",
    "    1. Compute the distance of each point to the clusters and assign it to the closest cluster.\n",
    "    2. Update the cluster centers as the mean of the data points assigned to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Cluster initialization\n",
    "\n",
    "First we will initialize K cluster centers randomly. Fill in the function to pick K points randomly from the given dataset as initial cluster centers and return them.\n",
    "\n",
    "*Hint:* you can randomly choose the index of the points, and then use these indices to select the data points. Look into `np.random.permutation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_centers(data, K):\n",
    "    \"\"\"\n",
    "    Randomly pick K data points from the data as initial cluster centers.\n",
    "    \n",
    "    Arguments: \n",
    "        data: array of shape (NxD) where N is the number of data points and D is the number of features (:=pixels).\n",
    "        K: int, the number of clusters.\n",
    "    Returns:\n",
    "        centers: array of shape (KxD) of initial cluster centers\n",
    "    \"\"\"\n",
    "    ### WRITE YOUR CODE HERE\n",
    "    indices = np.random.choice(data.shape[0], K, replace=False)\n",
    "    return data[indices]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let's use this function to first initialize 10 cluster centers and then visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of clusters, initialize their centers and display them.\n",
    "K = 10\n",
    "centers = init_centers(x_train, K)\n",
    "display_centers(centers, \"Initial {} cluster centers\".format(K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Cluster assignment\n",
    "Now let us implement some essential components of the k-means algorithm: \n",
    "* a function to compute the distance of each data point to all cluster centers `compute_distance()`\n",
    "* and a function to assign each data point to the closest cluster center `find_closest_cluster()`.\n",
    "\n",
    "The L2 (or Euclidean) distance between a data point $\\mathbf{x}_i$ and cluster center $\\mu_k$ is\n",
    "$$ d_{ik} = \\sqrt{\\sum_{d=1}^D \\left(\\mathbf{x}_i^{(d)} - \\mu_k^{(d)}\\right)^2 }, $$\n",
    "where $\\cdot^{(d)}$ means the *d*th coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance(data, centers):\n",
    "    \"\"\"\n",
    "    Compute the euclidean distance between each datapoint and each center.\n",
    "    \n",
    "    Arguments:    \n",
    "        data: array of shape (N, D) where N is the number of data points, D is the number of features (:=pixels).\n",
    "        centers: array of shape (K, D), centers of the K clusters.\n",
    "    Returns:\n",
    "        distances: array of shape (N, K) with the distances between the N points and the K clusters.\n",
    "    \"\"\"\n",
    "    N = data.shape[0]\n",
    "    K = centers.shape[0]\n",
    "\n",
    "    ### WRITE YOUR CODE HERE\n",
    "    distances = np.linalg.norm(data[:, np.newaxis, :] - centers, axis=2)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_cluster(distances):\n",
    "    \"\"\"\n",
    "    Assign datapoints to the closest clusters.\n",
    "    \n",
    "    Arguments:\n",
    "        distances: array of shape (N, K), the distance of each data point to each cluster center.\n",
    "    Returns:\n",
    "        cluster_assignments: array of shape (N,), cluster assignment of each datapoint, which are an integer between 0 and K-1.\n",
    "    \"\"\"\n",
    "    ### WRITE YOUR CODE HERE\n",
    "    cluster_assignments = np.argmin(distances, axis=1)\n",
    "    return cluster_assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the results after building clusters based on the initial cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of clusters are: 10\n",
      "The number of data points assigned to the cluster 1 is 117.\n",
      "The number of data points assigned to the cluster 2 is 72.\n",
      "The number of data points assigned to the cluster 3 is 97.\n",
      "The number of data points assigned to the cluster 4 is 147.\n",
      "The number of data points assigned to the cluster 5 is 63.\n",
      "The number of data points assigned to the cluster 6 is 170.\n",
      "The number of data points assigned to the cluster 7 is 168.\n",
      "The number of data points assigned to the cluster 8 is 31.\n",
      "The number of data points assigned to the cluster 9 is 187.\n",
      "The number of data points assigned to the cluster 10 is 148.\n"
     ]
    }
   ],
   "source": [
    "distances = compute_distance(x_train, centers)\n",
    "cluster_assignments = find_closest_cluster(distances)\n",
    "\n",
    "# The number of clusters should be the same as the number of centers, which is 10 here\n",
    "print('The number of clusters are:', len(centers))\n",
    "for i in np.arange(n_classes):\n",
    "    print(f'The number of data points assigned to the cluster {i+1} is {np.sum(cluster_assignments == i)}.')\n",
    "\n",
    "assert distances.shape == (x_train.shape[0], centers.shape[0]), \"distances is not of the correct shape!\"\n",
    "assert cluster_assignments.shape == (x_train.shape[0],), \"cluster_assignments is not of the correct shape!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the different clusters after initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We visualize 5 images in each cluster\n",
    "for k in range(K):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15,4))\n",
    "    fig.suptitle(\"Example data points assigned to cluster {}\".format(k+1))\n",
    "    cluster_points = x_train[cluster_assignments == k]\n",
    "    cluster_points_label = y_train[cluster_assignments == k]\n",
    "    for col in range(5):\n",
    "        display_digit(cluster_points[col], cluster_points_label[col], \"training\", axes[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Cluster update\n",
    "\n",
    "And now we need to update the cluster centers to be the mean of the data points assigned to each cluster.\n",
    "\n",
    "The center of cluster $k$ is given by\n",
    "$$ \\mu_k = \\frac1{N_k} \\sum_{i\\in C_k} \\mathbf{x}_i, $$\n",
    "where $C_k$ is the set of points assigned to cluster $k$, and $\\left\\lvert C_k \\right\\rvert = N_k$ is the number of such points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centers(data, cluster_assignments, K):\n",
    "    \"\"\"\n",
    "    Compute the center of each cluster based on the assigned points.\n",
    "\n",
    "    Arguments: \n",
    "        data: data array of shape (N,D), where N is the number of samples, D is number of features\n",
    "        cluster_assignments: the assigned cluster of each data sample as returned by find_closest_cluster(), shape is (N,)\n",
    "        K: the number of clusters\n",
    "    Returns:\n",
    "        centers: the new centers of each cluster, shape is (K,D) where K is the number of clusters, D the number of features\n",
    "    \"\"\"\n",
    "    ### WRITE YOUR CODE HERE\n",
    "    D = data.shape[1]\n",
    "    centers = np.zeros((K, D))\n",
    "\n",
    "    for k in range(K):\n",
    "        points_in_cluster = data[cluster_assignments == k]\n",
    "        if len(points_in_cluster) > 0:\n",
    "            centers[k] = np.mean(points_in_cluster, axis=0)\n",
    "        else:\n",
    "            centers[k] = data[np.random.randint(0, data.shape[0])]\n",
    "    \n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New centers after assigning the points to the previous centers\n",
    "new_centers = compute_centers(x_train, cluster_assignments, K)\n",
    "\n",
    "assert new_centers.shape == (K, x_train.shape[1]), \"new_centers have wrong shape!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the new centers look like? Let's visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_centers(new_centers, \"Updated {} cluster centers after 1 iteration\".format(K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** The visualization of the new cluster centers should appear as blurry numbers, some worse than others, while the original cluster centers were clear handwritten digits. Can you explain why?\n",
    "\n",
    "**A.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Full K-Means\n",
    "\n",
    "K-Means is an iterative algorithm that consists of two main steps after the center initialization:\n",
    "1. assigning each point $\\mathbf{x}_i$ to the nearest center $\\mu_k$\n",
    "2. updating each center $\\mu_k$ given the points assigned to it\n",
    "\n",
    "We iterate between these two steps until we converge or reach a defined maximum number of iterations. You can say that the algorithm has converged when the cluster centers do not move anymore. Fill in the function below to apply the full of K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(data, K, max_iter):\n",
    "    \"\"\"\n",
    "    Main function that combines all the former functions together to build the K-means algorithm.\n",
    "    \n",
    "    Arguments: \n",
    "        data: array of shape (N, D) where N is the number of data samples, D is number of features.\n",
    "        K: int, the number of clusters.\n",
    "        max_iter: int, the maximum number of iterations\n",
    "    Returns:\n",
    "        centers: array of shape (K, D), the final cluster centers.\n",
    "        cluster_assignments: array of shape (N,) final cluster assignment for each data point.\n",
    "    \"\"\"\n",
    "    # Initialize the centers\n",
    "    centers = init_centers(data, K)\n",
    "\n",
    "    # Loop over the iterations\n",
    "    for i in range(max_iter):\n",
    "        if ((i+1) % 10 == 0):\n",
    "            print(f\"Iteration {i+1}/{max_iter}...\")\n",
    "        old_centers = centers.copy()  # keep in memory the centers of the previous iteration\n",
    "\n",
    "        distances = compute_distance(data, old_centers)\n",
    "        cluster_assignments = find_closest_cluster(distances)\n",
    "        centers = compute_centers(data, cluster_assignments, K)\n",
    "\n",
    "        # End of the algorithm if the centers have not moved (hint: use old_centers and look into np.all)\n",
    "        if np.array_equal(centers, old_centers):  ### WRITE YOUR CODE HERE\n",
    "            print(f\"K-Means has converged after {i+1} iterations!\")\n",
    "            break\n",
    "    \n",
    "    # Compute the final cluster assignments\n",
    "    ### WRITE YOUR CODE HERE\n",
    "    \n",
    "    distances = compute_distance(data, centers)\n",
    "    cluster_assignments = find_closest_cluster(distances)\n",
    "    return centers, cluster_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/1000...\n",
      "K-Means has converged after 12 iterations!\n"
     ]
    }
   ],
   "source": [
    "# Let's try it on our training data\n",
    "K = 10\n",
    "max_iter = 1000\n",
    "final_centers, cluster_assignments = k_means(x_train, K, max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's visualize the final clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_centers(final_centers, \"Final {} cluster centers\".format(K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Testing K-Means\n",
    "\n",
    "Each data point now has a cluster assignment in $\\{0,..,9\\}$. However, these numbers do not really reflect the label of the cluster (because they are shuffled). For example an image may belong to the 0th cluster, but this does not mean the image depicts 0. But we have access to the true label of each data point in `y_train`. Let us now make use of these.\n",
    "\n",
    "If we want to assign the true label to each cluster after K-Means, we can do so via voting. We have now assigned to each cluster some data samples. The label of the cluster will be assigned as the most common label among these data samples (the mode). Each data point within the cluster will take the label of the cluster. (This is quite similar to the voting system of $k$-NN, except we consider the cluster here instead of the neighbors.)\n",
    "\n",
    "**Note:** K-Means is used for clustering, which is an unsupervised method. That means it does not use data labels and if you look at your implementation of K-Means, you should see that it only depended on the data points in `x_train`! We make here use of the true label simply to verify the results of our clustering and attribute pseudo-labels to our cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_labels_to_centers(centers, cluster_assignments, true_labels):\n",
    "    \"\"\"\n",
    "    Use voting to attribute a label to each cluster center.\n",
    "\n",
    "    Arguments: \n",
    "        centers: array of shape (K, D), cluster centers\n",
    "        cluster_assignments: array of shape (N,), cluster assignment for each data point.\n",
    "        true_labels: array of shape (N,), true labels of data\n",
    "    Returns: \n",
    "        cluster_center_label: array of shape (K,), the labels of the cluster centers\n",
    "    \"\"\"\n",
    "    K = centers.shape[0]\n",
    "    cluster_center_label = np.zeros(K, dtype=true_labels.dtype)\n",
    "\n",
    "    for k in range(K):\n",
    "        labels_in_cluster = true_labels[cluster_assignments == k]  \n",
    "        if len(labels_in_cluster) > 0:\n",
    "            counts = np.bincount(labels_in_cluster)  # Compte les occurrences de chaque label\n",
    "            cluster_center_label[k] = np.argmax(counts)  # Prend le label avec le plus d'occurrences\n",
    "    \n",
    "    return cluster_center_label\n",
    "\n",
    "def predict_with_centers(data, centers, cluster_center_label):\n",
    "    \"\"\"\n",
    "    Predict the label for data, given the cluster center and their labels.\n",
    "    To do this, it first assign points in data to their closest cluster, then use the label\n",
    "    of that cluster as prediction.\n",
    "\n",
    "    Arguments: \n",
    "        data: array of shape (N, D)\n",
    "        centers: array of shape (K, D), cluster centers\n",
    "        cluster_center_label: array of shape (K,), the labels of the cluster centers\n",
    "    Returns: \n",
    "        new_labels: array of shape (N,), the labels assigned to each data point after clustering, via k-means.\n",
    "    \"\"\"\n",
    "    ### WRITE YOUR CODE HERE\n",
    "    distances = compute_distance(data, centers)\n",
    "    cluster_assignments = find_closest_cluster(distances)\n",
    "    new_labels = cluster_center_label[cluster_assignments]\n",
    "    \n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After assigning labels to our data points, we can check for accuracy as the percent of correct labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(pred_labels, gt_labels):\n",
    "    correct_predictions = np.sum(pred_labels == gt_labels)\n",
    "    accuracy = (correct_predictions / len(gt_labels))\n",
    "    return accuracy* 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run and test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/1000...\n",
      "Iteration 20/1000...\n",
      "Iteration 30/1000...\n",
      "Iteration 40/1000...\n",
      "Iteration 50/1000...\n",
      "K-Means has converged after 50 iterations!\n",
      "Accuracy on the train set is 59.8%.\n",
      "Accuracy on the valid set is 61.8%.\n"
     ]
    }
   ],
   "source": [
    "K = 10\n",
    "max_iter = 1000\n",
    "final_centers, cluster_assignments = k_means(x_train, K, max_iter)\n",
    "cluster_center_label = assign_labels_to_centers(final_centers, cluster_assignments, y_train)\n",
    "pred_labels = predict_with_centers(x_train, final_centers, cluster_center_label)\n",
    "print(f\"Accuracy on the train set is {accuracy_fn(pred_labels, y_train):.1f}%.\")\n",
    "\n",
    "# Let's check results on the validation set as well!\n",
    "pred_labels_val = predict_with_centers(x_val, final_centers, cluster_center_label)\n",
    "print(f\"Accuracy on the valid set is {accuracy_fn(pred_labels_val, y_val):.1f}%.\")\n",
    "acc = accuracy_fn(pred_labels_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*After we choose a value for K based on the performance on the validation set*, we can try to see how this label assignment to clusters perform on unseen data. For this, we look at the accuracy on the held out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set is 59.6%.\n"
     ]
    }
   ],
   "source": [
    "pred_labels_test = predict_with_centers(x_test, final_centers, cluster_center_label)\n",
    "print(f\"Accuracy on the test set is {accuracy_fn(pred_labels_test, y_test):.1f}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize some data points with their predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 10 points in the training set\n",
    "rand_idx = np.random.permutation(x_train.shape[0])[:10]\n",
    "samples = x_train[rand_idx]\n",
    "pred_y = pred_labels[rand_idx]\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15,5))\n",
    "fig.suptitle(\"Predictions\".format(K))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    display_digit(samples[i], pred_y[i], \"inferred\", ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the corresponding cluster centers and their label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_row = int(np.ceil(K / 5))\n",
    "fig, axes = plt.subplots(n_row, 5, figsize=(15, 2.5 * n_row))\n",
    "fig.suptitle(\"Final {} cluster centers and their labels\".format(K))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < K:\n",
    "        display_digit(final_centers[i], cluster_center_label[i], \"inferred\", ax=ax)\n",
    "    else:\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. K-Means clustering for images\n",
    "Now that we have a working K-Means, let's try something a bit different that you may have seen in the lectures: cluster the pixel colors in an image.\n",
    "\n",
    "It consists in considering each pixel a 3D vector, where the coordinates are actually its RGB values, and performing clustering on them. Then, we can replace each pixel in the image by the center of the cluster it belongs to.\n",
    "\n",
    "The resulting images will then only need to store K colors and a single integer per pixel (representing cluster assignment). This can be seen as a form of image compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load and visualize the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.array(Image.open(\"img/lenna.png\"))\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.imshow(image)\n",
    "plt.title(\"Original image\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we consider all pixels as points and run K-Means on them. Let's also make a function to visualize the results.\n",
    "\n",
    "Complete the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_on_image(image, K, max_iter):\n",
    "    \"\"\"\n",
    "    Perform K-Means on the pixel of an image.\n",
    "\n",
    "    Arguments:\n",
    "        image: array of shape HxWx3, the RGB image\n",
    "        K: int, the number of cluster\n",
    "    Returns:\n",
    "        pixel_centers: array of shape Kx3, centers of the cluster\n",
    "        pixel_assignments: array of shape HxW, cluster assignment of each pixel\n",
    "    \"\"\"\n",
    "    ### WRITE YOUR CODE HERE\n",
    "    # First, reshape the data as an NxD array, with D=3.\n",
    "    H = image.shape[0]\n",
    "    W = image.shape[1]\n",
    "    pixels = np.reshape(image,(H*W,3))\n",
    "\n",
    "    # Run K-Means on the pixels.\n",
    "    pixel_centers, pixel_assignments = k_means(pixels, K, max_iter)\n",
    "\n",
    "    # Reshape the pixel assignments back to the 2D image shape. \n",
    "    # Hint: pixel_assignments have a single value for each pixel, not RGB!\n",
    "    pixel_assignments = np.reshape(pixel_assignments,(H,W))\n",
    "\n",
    "    return pixel_centers, pixel_assignments\n",
    "\n",
    "def make_kmeans_image(pixel_centers, pixel_assignments):\n",
    "    \"\"\"\n",
    "    Make an RGB image out of the clustered colors and pixel assignments.\n",
    "\n",
    "    Arguments:\n",
    "        pixel_centers: array of shape Kx3, centers of the cluster\n",
    "        pixel_assignments: array of shape HxW, cluster assignment of each pixel\n",
    "    Returns:\n",
    "        kmeans_image: array of shape HxWx3, resulting image after K-Means.\n",
    "    \"\"\"\n",
    "    K = pixel_centers.shape[0]\n",
    "    H, W = pixel_assignments.shape\n",
    "    \n",
    "    ### WRITE YOUR CODE HERE\n",
    "    kmeans_image = pixel_centers[pixel_assignments]\n",
    "    return kmeans_image.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/1000...\n",
      "Iteration 20/1000...\n",
      "Iteration 30/1000...\n",
      "K-Means has converged after 39 iterations!\n"
     ]
    }
   ],
   "source": [
    "K = 6 # try different number of clusters\n",
    "max_iter = 1000\n",
    "\n",
    "pixel_centers, pixel_assignments = kmeans_on_image(image, K, max_iter)\n",
    "kmeans_image = make_kmeans_image(pixel_centers, pixel_assignments)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "fig.suptitle(f\"Results with K={K}\")\n",
    "axs[0].imshow(image)\n",
    "axs[0].set_title(\"Original image\")\n",
    "axs[1].imshow(kmeans_image)\n",
    "axs[1].set_title(\"Image after K-Means\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Written questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** (MCQ) Consider the K-Means algorithm. Select all the correct statement(s) below. \n",
    "1. The K-Means algorithm learns the number of clusters while optimizing their position.\n",
    "2. K-Means does not require labels for the training data.\n",
    "3. Different cluster initializations always lead to the same final solution.\n",
    "4. The data features should be of similar scale.\n",
    "\n",
    "**A1.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.** (SCQ) We use K-Means with the Euclidean distance to cluster the 2D points represented by the crosses below. The initial cluster centers are the two circles. After convergence, what are the most plausible coordinates for the final cluster centers?\n",
    "\n",
    "<img src=\"img/kmeans_q2.png\" width=600>\n",
    "\n",
    "* $(-0.67, -0.67)$ and $(6.4, 0.8)$\n",
    "* $(1.6, -0.2)$ and $(7.33, 1)$\n",
    "* $(-0.67, -0.67)$, $(5, 0.5)$ and $(7.33, 1)$\n",
    "* $(1.6, -0.2)$ and $(6.4, 0.8)$\n",
    "\n",
    "**A2.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.** Consider the two scenarios below, can you apply K-Means in their cases?\n",
    "1. After a dancing tournament, your machine learning team has extracted the 3D poses of all dancers' moves. The pose for one \"dancing move\" can be represented as a vector of J*3 values that represent the 3D position of J joints (such as the elbows, knees, shoulders, etc.) of the dancer. You would like to regroup the similar dancing moves in this data.\n",
    "2. Your little brother has had a lemonade stand for the past 2 months. He kept track of how much he sold, the daily weather, the number of people passing by, and the number of people that stopped at his stand. He asks you to use machine learning to predict how much he might sell on any given day assuming you know the rest of the information.\n",
    "\n",
    "**A3.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.** K-Means with Euclidean distance works better on scaled data, e.g., when each dimension is normalized between 0 and 1. Why? In this exercise we haven't done any normalization. Can you see why it was not needed?\n",
    "\n",
    "**A4.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57d8526aca424c3deaf369d9657bf7dc80e220f5e8c5420c16cbdbf3db48769f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
