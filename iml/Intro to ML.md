## Norms

$L_1$ Manhattan distance : $sum |x_d - x_d|$.
$L_2$ Euclidean distance : $sqrt(sum (x_d - x quote.single_d))^2$

## KNNs (K Nearest Neighbors)

#### Un algo simple

On prend le voisin le plus proche et on le choisit comme r√©sultat.

**Voronoi cells** : on cr√©√© des cellules autour de chaque point de telle sorte √† ce qu'on puisse voir visuellement.
![[assets/norms.png]]

#### K-nearest neighbors

- on trouve les k voisins les plus proches
- on prend la majorit√© parmis ces voisins pour d√©cider
### Entra√Ænements et ensemble de tests

On d√©finit toujours un ensemble de test pour juger les performances du mod√®le, et un ensemble d'entra√Ænement pour faire apprendre au classificateur.

On veut toujours choisir le bon degr√© de "conformisation aux donn√©es d'entra√Ænement". On ne veut pas que le mod√®le ne puisse que reconnaisse que les donn√©es d'entra√Ænement mais qu'il se rapproche de la courbe r√©elle.

![[assets/overfitting.png]]
#### Am√©liorer son dataset

##### Data reduction

On veut absolument r√©duire le nombre de donn√©es de notre ensemble de donn√©es quand on applique les K-NN parce qu'on doit √† chaque fois comparer avec **tous** les voisins. Pour √ßa on choisit des repr√©sentants (des **prototypes**). Mais il y a plusieurs techniques pour faire √ßa :
- avec le centre de gravit√© (mais √ßa ne marche pas tjrs, par exemple si on a un cercle rouge entour√© par un cercle vert, ils ont le m√™me centre de gravit√© !)
- avec l'algorithme des **condensed** nearest neighbors (de meilleurs fronti√®res **et** plus rapide √† ex√©cuter car moins de comparaisons) :
	- on a un training set 1, 2, (bleu) 3, 4, (rouge) 5 (vert).
	- On initialise `P = {1}` (random).
	- Ensuite, on choisit p. ex. le 2. Le plus pr√®s de 2 dans P est 1, qui a la m√™me classe, donc on jette le 2.
	- On choisit le 3. Quel est le plus pr√®s du 3 dans `P` ? C'est 1, qui a une classe diff√©rente, donc on garde C.
	- On choisit le 4. Quel est le plus pr√®s du 4 dans `P` ? C'est 3, qui a une classe indentique, donc on jette 4.
	- etc. cet algo n'a pas toujours de sens.
##### Normalisation

Vous appliquez le KNN √† un dataset comme ceci : 
‚Ä¢ Age: Ranges from 0 to 100
‚Ä¢ Income: Ranges from $0 to $1,000,000
‚Ä¢ Binary Gender: Encoded as 0 or 1

S'il y a une diff de $1000 entre A et B, le mod√®le va consid√©rer √ßa plus important qu'une diff de 20 ans entre les deux ! On doit donc normaliser (garder la m√™me distribution mais faire un rescaling).
##### Corriger unbalanced dataset

- on peut enlever des points pour r√©tablir l'√©quilibre
- on peut ajouter un poids plus fort aux points 
- on peut ajouter des points synth√©tiques pour compenser

### Greedy k-NN Graph construction

Idea: connect everyone to a few people (randomly) and look at friends of friends.
p. exemple :
- on connecte Alice √† 3 √©trangers.
- on regarde dans les amis des amis d'Alice et on compute leur similarity score.
- on prend les premiers et on cr√©√© un nouveau "voisinage" √† partir de √ßa.
- et on r√©p√®te tant que le nombre de changements est $>epsilon$.
## K-means

- un cluster est un ensemble de points ${x_(i_1^k), ..., x_(i_(n^k)^k)}$
- $mu_k$  est le centre de masse du cluster $k$

Nous voulons que les distances entre les points au sein d'un cluster soient petites et que les distances entre les clusters soient larges.

$$"on veut minimiser : " sum_(k = 1)^K sum_(j = 1)^(n_k) (x_(i_j)^k - mu_k)^2$$
**Comment trouver les centres de masse** ?

- on initialise les centres de masse √† une position al√©atoire
- jusqu'√† ce que √ßa ne change plus
	- on assigne chaque point au centre de masse le plus proche (en calculant la distance euclidienne -- un point ne peut √™tre associ√© qu'√† un centre de masse)
	- on met √† jour chaque $mu_k$ en fonction de la moyenne des points associ√©s

$arrow$ √ßa ne marche pas toujours! on doit essayer avec plusieurs seeds (plusieurs positions al√©atoires au d√©but) et prendre celle qui √† le meilleur r√©sultat en termes de distance au carr√©


## Mod√®les lin√©aires

On a deux types de mod√®les lin√©aires.
### Mod√®le de r√©gression lin√©aire (le plus simple)

Utilis√©s pour pr√©dire une valeur continue, par exemple pour pr√©dire le prix d'une maison en fonction de ses caract√©ristiques.

$$ y = a_0 + a_1 x_1 + ... + a_i x_i$$
$y$ est la valeur de sortie, $x_i$ est la $i$√®me feature, $a_i$ est le poids associ√© √† la $i$-√®me feature.

**Comment d√©terminer les param√®tres du mod√®le ?**

Notre fonction de loss est une fonction de distance entre le $y_"pred"$ trouv√© et le $y_"ground"$ :
$$ "Loss"_"MSE" = 1/m sum_(i = 1)^m (y_i - y_(i, "pred"))^2 $$
### Mod√®le de logistic r√©gression binaire

Utilis√©s pour des probl√®mes de classification binaire. Par exemple,  si on veut classifier deux types de poisson √† partir d'un vecteur de features comme $mat("lumi√®re"; "taille")$.

On applique la fonction sigmo√Øde :
$$ p(y = 1|x_1, x_2, ..., x_i) = 1/(1 + e^(-(a_0 + a_1 x_1 + ... + a_i x_i)) $$
Et on d√©termine $$ y_"pred" = cases(1 "si" p(y = 1|x) >= 0.5, 0 "sinon")$$
**Comment d√©terminer les param√®tres du mod√®le ?**

Ici on utilise la fonction de loss d'entropie crois√©e ! En fait, elle vient de ce qu'on a vu en probabilit√©s et statistiques avec la m√©thode du likelihood :

> [!question] d'o√π vient la cross entropy loss ?
> 
> On a donc deux coins, deux distributions :
> - le vrai coin **1**, avec comme distribution $1/2$, $1/2$
> - notre coin mod√©lis√© **2**, qui cherche √† se rapprocher de la distribution du coin 1 (parce qu'on ne conna√Æt pas la distribution du coin **1** bien s√ªr, on cherche √† s'en rapprocher √† partir de ce qu'on observe). On d√©finit au d√©but nos poids √† $0.55$, $0.45$ par exemple.
>  
>  Avec des donn√©es d'entra√Ænement, comme l'observation `H H T H T`, on peut calculer :
>  $$ t = P("observation" | "vrai coin")/P("observation" | "coin mod√©lis√©") = P_"v"/P_"c" = (p_1^(N_H) dot p_2^(N_T))/(q_1^(N_H) dot q_2^(N_T)) $$
>  avec $p_1, q_1$ la probabilit√© d'avoir un head avec le coin 1 et le coin 2 respectivement et m√™me chose pour $p_2, q_2$ pour tail, puis $N_H, N_T$ le nombre de head et tail observ√©s respectivement.
>  
>  On normalise $T = (P_"v"/P_"c")^(1/n)$ puis on applique le log : $T_"log" = 1/n log(P_"v"/P_"c")$, puis les prop du log :
>  $$ T_"log" = N_H/N log(p_1) + N_T/N log(p_2) - N_H/N log(q_1) - N_T/N log(q_2) $$
>  
>  L√†, on peut faire la simplification suivante : avec $N$ qui va √† l'infini $N_H/N$, va √™tre tr√®s pr√®s de la vraie probabilit√© d'avoir un head, et pareil pour $N_T/N$ donc :
>  $$ lim_((N, N_H, N_T) -> infinity) T_"log" = p_1 log(p_1) + p_2 log(p_2) - p_1 log(q_1) - p_2 log(q_2) $$
>  Et en r√©arrageant les termes :
>  $$ lim_((N, N_H, N_T) -> infinity) T_"log" = p_1 log(p_1/q_1) + p_2 log(p_2/q_2) = D_(K L) (P | | Q) = sum_i P(i) log(P(i)/Q(i)) $$
> 
> > [!question] Pourquoi $D_(K L )$ est une bonne mesure de la distance ?
> > C'est une bonne mesure de la distance entre les deux distributions $P$ et $Q$.
> > - Si les deux sont exactement √©gales, le ratio sera de **1** et le log sera √©gal √† z√©ro.
> > - Si $Q$ est tr√®s diff√©rent de $P$, alors forc√©ment, certains $q_i$ seront plus petits que
> >   certains $p_i$, ce qui fera augmenter tr√®s fortement la somme (en effet les termes de $q_i$ un peu plus grand que ceux de $p_i$, eux, diminuent moins la somme √† cause de la forme du $log$). c'est pour √ßa d'ailleurs que $D_(K L) >= 0$ !
> >   
> >   $$ D_(K L) (P | | Q) = sum_i ‚ÄãP(i)log(Q(i)/P(i)) ‚Äã>= log (sum_i ‚ÄãP(i) dot Q(i)/P(i))‚Äã \ = log sum_i P(i)= log 1 = 0 $$
> >   
 > ![[assets/image-35.png|198x71]]
>  
>  > [!info] tout ce √† quoi on vient d'arriver tient aussi avec plus de deux classes !
> 
> Nous on veut donc minimiser cette distance $D_(K L)$.
> 
> $$ D_(K L) (P_"true" | P_"pred") = sum_y P_"true" (y | x_i) log((P_"true" (y | x_i))/(P_"pred"(y | x_i ; " " theta))) \ = sum_y P_"true" (y | x_i) log(P_"true" (y | x_i)) - P_"true" (y | x_i) log(P_"pred" (y | x_i ; " " theta)) $$
> 
> Or ici tout le premier terme est inutile, il ne d√©pend pas de $theta$ !
> 
> On retrouve donc $$ "argmin"_theta D_(K L) (P_"true" | | P_"pred") = "argmin"_theta - sum_y P_"true" (y | x_i) log(P_"pred" (y | x_i; " " theta)) $$
> 
> La formule de l'entropie crois√©e ! 
#### Revenir √† l'id√©e de base

On d√©cide d'utiliser une ligne droite $C$ et on cherche les constantes $w_x, w_y$ telles que donn√©s $b, l$ , on puisse trouver de bons $-1$ et $1$ en appliquant $y$. Plus tard, on va utiliser une sigmoid pour pr√©dire une probabilit√© et non plus seulement $-1$ et $1$.

![[assets/image-8.png]]

**Rappel d√©finition d'une ligne en 2D**:
![[assets/image-10.png|499x255]]
On peut la normaliser en posant $a^2 + b^2 = 1$. Cela rend le vecteur normal plus simple.

![[assets/image-11.png|511x392]]

### Perceptron

Nous voulons minimiser : $$ E(\tilde{w}) = - \sum_{n = 1}^{N} sign(\tilde{w} \cdot \tilde{x_n}) t_n $$
o√π $t_n$ est la vraie valeur.

**Comment ?**
- commencer par d√©finir $\tilde{w}_1$ √† 0.
- de fa√ßon it√©rative, choisir un indice $n$
	- si $x_n$ est correctement classifi√©, ne rien faire.
	- sinon, $\tilde{w}_{t + 1} = \tilde{w_t} + t_n \tilde{x_n}$

**Centered perceptron** : on shift tous nos points de telle sorte √† ce que notre d√©cision boundary passe par l'origine.

**Convergence Theorem** : dans un probl√®me de classification, s'il existe une marge $gamma$ telle qu'on ait une classification parfaite, le perceptron algorithm fait au plus $R^2/gamma^2$ erreurs (avec $R = max(|x_n|)$ ).

Parfois, pour un $gamma$ tr√®s petit ou quand la classification ne peut pas √™tre parfaite, on ne doit pas aller point par point dans l'ordre v√©rifier s'il est correctement classifi√©, mais les v√©rifier de fa√ßon **al√©atoire**.

Un probl√®me avec le perceptron, c'est qu'on dit que le r√©sultat est $-1$ ou $1$. Donc l'algorithme consid√®re que ces deux r√©sultats sont √©quivalents, de m√™me que la ligne en diagonale qui serait bien meilleure :

![[assets/image-12.png|267x219]]

Ce qu'il faudrait, c'est avoir une fonction plus smooth :

![[assets/image-13.png|416x225]]


![[assets/image-14.png|385x294]]

![[assets/image-15.png|396x263]]

Sensitive to outliers. We have to accept that some points get missclassified.
On pr√©f√®re la s√©lection du bas !

![[assets/image-43.png|409x251]]

On d√©finit la marge comme ceci. On veut maximiser la marge, quitte √† ignorer quelques outliers. La regression logistique ne garantie pas du tout √ßa.

#### Un probl√®me + simple : supposons pas d'outliers, maximisons la marge

√âtant donn√© un training set $$ {(x_n, t_n)} " avec" t_n in {-1, 1}$$ et une solution telle que $$ forall n, t_n (w dot x_n) >= 0 $$On peut √©crire la distance √† la d√©cision boundary non sign√©e :
$$ d_n = t_n (w dot x_n)/(||omega||) $$
$$w* = "argmax"_w min(t_n dot (w dot x)/(||w||))$$

Un probl√®me √©quivalent est : (preuve dans le cours ?)
$$ w* = "argmin"_w 1/2 ||w||^2 $$

![[assets/image-44.png]]

![[assets/image-45.png]]

Avec un grand C on p√©nalise les miss classifications.

![[assets/image-46.png]]
### Mod√®le de logistic regression multiclass

On peut soit utiliser $D_(K L)$ soit softmax. 

## Que faire quand on a des donn√©es non s√©parables lin√©airement ?

On peut combiner des classifiers lin√©aires.

![[assets/image-47.png]]
![[assets/image-49.png]]
![[assets/image-52.png]]
![[assets/image-53.png]]


## Mesurer les performances d'un mod√®le

### Matrice de confusion

C'est un tableau qui compare les pr√©dictions par rapport aux valeurs r√©elles.

|              | **Pr√©dit : 0**          | **Pr√©dit : 1**          |
| ------------ | ----------------------- | ----------------------- |
| **R√©el : 0** | **TN** (Vrais N√©gatifs) | **FP** (Faux Positifs)  |
| **R√©el : 1** | **FN** (Faux N√©gatifs)  | **TP** (Vrais Positifs) |
### Pr√©cision

R√©pond √† la question : **"Parmi les pr√©dictions positives du mod√®le, combien sont correctes ?"**

$$ "Pr√©cision = " = "# true positive"/("# true positive" + "# true negative") $$

### Accuracy

Le **recall** (ou **sensibilit√©**) est une m√©trique qui mesure la capacit√© d‚Äôun mod√®le √† identifier correctement les **cas positifs**.

$$ "exactitude" = "# vraies pr√©dictions"/"# total des pr√©dictions"$$|**M√©trique**|**Formule**|**Question qu'elle r√©pond**|**Quand l'utiliser ?**|
|---|---|---|---|

|               |                                                            |                                                             |                                                                                             |
| ------------- | ---------------------------------------------------------- | ----------------------------------------------------------- | ------------------------------------------------------------------------------------------- |
| **Pr√©cision** | $$(T P)/(T P + F P)$$‚Äã                                     | "Parmi les pr√©dictions positives, combien sont correctes ?" | Quand les **faux positifs** sont co√ªteux (ex : filtrage de spam, faux diagnostics m√©dicaux) |
| **Accuracy**  | $$"# pr√©dictions correctes"/("# total des pr√©dictions")$$‚Äã | "Globalement, combien de pr√©dictions sont correctes ?"      | Si les classes sont **√©quilibr√©es** (ex : reconnaissance faciale)                           |
| **Recall**    | $$(T P)/(T P + F N)$$‚Äã                                     | "Parmi tous les vrais positifs, combien ont √©t√© d√©tect√©s ?" | Quand les **faux n√©gatifs** sont critiques (ex : d√©tection de maladies, s√©curit√©)           |
Le **F1 Score** est une moyenne harmonique entre la pr√©cision et le recall, ce qui signifie qu‚Äôun mod√®le avec une pr√©cision tr√®s √©lev√©e mais un recall faible (ou inversement) aura un **F1-score faible**. 

$$ F_1 = 2 dot ("Pr√©cision" dot "Recall")/("Pr√©cision + Recall") $$

### MSE Loss function

$$ "MSE" = 1/N sum_(i = 1)^(N_t) (y_"pred-i" - y_i)^2 $$
Comment le d√©river ?
$$ L = 1/N sum_i (X W - Y)_i^2 \ = 1/N sum_i (X W - Y)_i^T (X W - Y)_i \ => nabla_W L = 1/N X^T (X W - Y) $$

## Arbres de d√©cisions

- **root node** : repr√©sente la population enti√®re et se divise en sous-ensembles
- **splitting** : diviser un noeud en plusieurs autres noeuds en se basant sur un crit√®re
- **decision node** : quand un noeud a plusieurs enfants
- **leaf/terminal node** : quand il n'a pas d'enfants
- **pruning** : quand on enl√®ve un decision node de l'arbre


La difficult√© est de choisir les d√©cisions rules. Pour √ßa, on va it√©rer sur toutes les d√©cisions rules possibles, et voir l'information gagn√©e.

**Comment mesurer l'information gagn√©e ?** On peut utiliser une mesure comme Gini impurity qui va mesurer √† quel point notre groupe est compos√©e d'une classe unique (est pur).
Information gagn√©e : impurity du parent - impurity des enfants (weighted)

On a souvent de l'over fitting : on d√©finit souvent un max depth.

## üå≤ 2. **Random Forests**

### üß† Id√©e

Un **ensemble d‚Äôarbres de d√©cision** construits sur des **sous-√©chantillons al√©atoires** des donn√©es (avec remplacement, c‚Äôest le **bagging**).

![[image-33.png]]

Arrention √† bien s√©lectionner **avec replacement**, sinon tous les arbres se ressembleraient et on ne gagnerait pas beaucoup.

Chaque arbre est diff√©rent car :
- Il est entra√Æn√© sur un jeu de donn√©es diff√©rent. (**bootstrapped dataset**)
- √Ä chaque n≈ìud, on ne consid√®re qu‚Äôun **sous-ensemble al√©atoire de features**.
### ‚úÖ Avantages

- **Moins d‚Äôoverfitting** que les arbres seuls.
- **Robuste** aux bruits.
- Peut estimer la probabilit√© d'appartenance √† une classe.

Pour classifier, on demande √† chaque tree de classifier et on prend le plus de votes. On aggrege les donn√©es. **Bootstrapped dataset + aggregation = bagging**.

Les donn√©es qui ne finissent dans aucun arbre s'appellent des **Out-of-Bag Datasets**. On peut mesurer l'accuracy de notre mod√®le en utilisant les out-of-bags dataset (on fait pr√©dire √† notre arbre les valeurs de out of bags).

---

## üöÄ 3. **Gradient Boosted Trees (GBT, ou XGBoost, LightGBM)**

### üß† Id√©e

On ajoute **des arbres faibles un par un** pour corriger les erreurs du mod√®le pr√©c√©dent. C‚Äôest une approche **s√©quentielle**.

- √Ä chaque √©tape, on construit un nouvel arbre pour pr√©dire **les r√©sidus (erreurs)** du mod√®le courant.
    
- On **ajuste progressivement**, un peu comme un apprentissage par descente de gradient.
    

### üîß D√©tails :

- On utilise un **taux d‚Äôapprentissage (shrinkage)** pour √©viter l‚Äôoverfitting.
    
- Tr√®s puissant, souvent **top 1 sur Kaggle**, mais plus sensible aux r√©glages.


## Descente de gradient

### D√©finition

- on a une fonction de loss, mais on ne peut pas trouver son minimum directement
- on d√©rive, puis on calcule $"step_size" = "d√©riv√©e" * "learning rate"$
- on se d√©place dans cette direction

Comme √ßa, on fait des "baby step" quand on est proche de la solution, puis de grosses steps quand la d√©riv√©e est √©lev√©e. On s'arr√™te quand la step size est tr√®s proche de z√©ro.

![[image-56.png|379x182]]

![[image-95.png|527x277]]

exemple d'une cost function avec 2 weights, on essaye de trouver le minimum
la cost function est donc une fonction qui prend les weights en entr√©es, la run sur le modele et donne un nombre (l'erreur) pour chaque paire.

> [!tip] Hyperparam√®tre √† bien choisir : le learning rate
> 
> si trop petit, √ßa va prendre trop de temps, si trop grand, √ßa va diverger
> 
> ![[image-57.png|340x153]]
> ![[image-58.png|347x179]]
> 
> Pour trouver la bonne learning rate, on utilise Grid search.

> [!danger] La descente de gradient ne donne pas toujours la bonne solution!
> 
> ![[image-58.png|347x179]]
> 
> Dans certains cas, on ne va jamais converger vers la bonne solution! Heureusement, la fonction de loss pour une linear regression est convexe (comme $x^2$)! donc on trouvera la bonne solution.

> [!tip] Normaliser les features pour gagner du temps!
> ![[image-60.png]]
> 
> En fonction du scaling des features, on peut arriver plus ou moins vite au r√©sultat (voir l'image).

La fonction n'a pas besoin d'√™tre diff√©rentiable partout, juste la ou on veut l'optimiser.
#### Batch (or full) gradient descent

> [!question] Comment l'impl√©menter ?
> 
> ![[image-61.png]]
> 
> On calcule la d√©riv√©e partielle par rapport a chaque parametre $theta$. (la formule vient de la d√©riv√©e de MSE).
> 
> C'est lent ! on doit multiplier une matrice de la taille des samples de training
> ![[image-62.png]]

#### Conjugate Gradient Method

> [!question] Comment √ßa marche ?
> 
> La m√©thode du gradient conjugu√© est une **alternative plus rapide au gradient classique** pour r√©soudre des **syst√®mes lin√©aires** du type $A x = b$.
> 
> Contrairement √† la descente de gradient simple, elle utilise l‚Äôid√©e de **"directions conjugu√©es"**, qui sont meilleures que les directions de gradient pour atteindre le minimum en moins d‚Äôit√©rations. Cela garantit qu'on ne "revient jamais en arri√®re" sur une direction d√©j√† optimis√©e.
> 

#### Lagrangian optimization

> [!question] Comment √ßa marche ?
> 
> Quand on optimise une fonction sous contraintes (par exemple, respecter un budget, une limite physique, etc.), certaines contraintes sont **actives** (juste au bord de ce qui est permis), et d‚Äôautres **non actives** (elles ne sont pas tendues).
> 
> par pas tendues, on veut dire qu'elle ne respecte pas exactement l'√©galit√©, alors que si c'est le cas, pour trouver la contrainte on est oblig√©s de se d√©placer le long de la droite.
> 
> pas active => coefficients z√©ro.

## Multi layer perceptron MLPS

> [!tip] Activation functions
> 
>![[image-93.png|512x252]]
>
> Permet de passer d'une valeur num√©rique (li√©e a une somme d'entr√©es multipli√©es par des poids, a une valeur entre deux bornes).

> [!tip] Transition d'un layer a un autre
> 
> ![[image-94.png|598x352]]
> 
> $equiv a^(1) = sigma(W a^(0) + b)$
> etc. en boucle

> [!tip] Comment entra√Æner le modele ? TODO: backpropagation
> 
> https://www.youtube.com/watch?v=tIeHLnjs5U8

## CNN Convolutional Neural Networks

On utilise les CNN pour des t√¢ches difficiles comme reconna√Ætre une photo de maison.

> [!danger] Mais pourquoi ne pas simplement le faire avec un Artificial Neural Network ?
> 
> On aurait besoin de trop de poids, c'est juste limite pour le MNIST.

![[image-121.png|518x240]]

> [!tip] L'id√©e du CNN
>  
>  Contrairement au MLP, les layers des CNN ne sont pas tous connect√©s a chaque pixel de l'image mais qu'a une partie. Chaque neurone de la deuxi√®me couche convolutionnelle est connect√© √† une petite r√©gion de la couche pr√©c√©dente, ce qui permet au r√©seau de d√©tecter d‚Äôabord des caract√©ristiques simples, puis de les combiner progressivement en motifs plus complexes. Cette structure hi√©rarchique, similaire √† celle des images r√©elles, rend les CNN particuli√®rement efficaces pour la reconnaissance d‚Äôimages.

> [!tip] Filtres et feature maps
> 
> Les poids d‚Äôun neurone dans une couche convolutionnelle peuvent √™tre vus comme une petite image (matrice) de la m√™me taille que son **champ r√©ceptif** (receptive field).
> 
> Par exemple un filtre de **7√ó7** avec **des 1 au centre vertical** d√©tectera uniquement les **lignes verticales**.
> 
> Dans une couche convolutionnelle, tous les neurones appliquent plusieurs filtres en parallees. mais elles partagent **tous** les m√™mes poids pour chaque filtre ! Chaque neurone, pour chaque filtre :
>- Regarde une petite r√©gion de l‚Äôimage (le **champ r√©ceptif**),
>- Applique **le m√™me filtre** (les m√™mes poids),
>- Produit **une valeur** qui dit : _¬´ est-ce que le motif que je cherche est pr√©sent ici ? ¬ª_ 
>  
>Ce partage des poids rend le modele beaucoup plus petit.
> 
>Le r√©sultat est appel√© une **carte de caract√©ristiques** (_feature map_), qui montre les endroits de l‚Äôimage o√π le filtre d√©tecte quelque chose d‚Äôimportant.

Pas besoin de programmer manuellement ces filtres :
- Pendant **l‚Äôentra√Ænement**, le r√©seau apprend **tout seul** quels filtres sont les plus utiles pour la t√¢che (comme la reconnaissance d‚Äôimage).
- Les couches sup√©rieures du r√©seau vont alors combiner ces filtres simples (lignes, bords...) pour reconna√Ætre des motifs plus complexes (formes, objets...).

![[image-122.png]]

Formule de la convolution (sur les)
![[image-123.png]]

![[image-125.png|546x299]]

- **Stride** $s$ : nombre de pixels √† sauter entre chaque application du filtre
- **Padding** $p$ : nombre de pixels √† ajouter autour de l‚Äôimage pour g√©rer les bords

![[image-126.png]]

Les 2p c'est parce qu'on ajoute p pixels en haut, et p pixel en bas. H_in - H_k parce que plus le filtre est grand plus on doit commencer loin du bord pour √™tre dans une plage valide.

> [!question] Calculer le nombre d'op√©rations ?
> 
> Chaque output pixel est le r√©sultat d'une convolution (c'est-a-dire du dot product entre une r√©gion de taille $H_K‚Äã dot W_K‚Äã dot C_"in"$ et un noyau de la m√™me taille), et il faut le faire pour chaque pixel de sortie soit $H_K‚Äã dot W_K‚Äã dot C_"in" dot H_"out"‚Äã dot W_"out" dot C_"out"$ multiplications.
> 
> Si on veut comparer avec un fully-connected layer, ou la taille du kernel est la taille de l'image, on voit que c'est bien plus petit.

> [!tip] average pooling
> 
> L'**Average pooling** consiste √† **diviser l'image en petites r√©gions (souvent carr√©es)** et √† **calculer la moyenne des valeurs** dans chacune de ces r√©gions.
> 
> **Max pooling** divise une image (ou carte de caract√©ristiques) en petits blocs (ex. 2√ó2) et **retient uniquement la valeur maximale** de chaque bloc.

# Transformers

![[image-151.png]]

### √âtape 1 : s√©parer en chunks "flattening" et transformer ces chunks (tokens) en embeddings

Prendre une image, une phrase, etc. ou autre et "l'aplatir", la transformer en petits chunks (g√©n√©ralement des syllables, mots, etc. ou petites parties d'images).

texte : chunks sont appel√©s tokens
image : chuncks sont appel√©s patches

Ensuite, on prend chaque chunk, et on obtient un vecteur √† partir de la **embedding matrix** (qui contient d√©j√† un mapping 1-1 entre chaque token et son embedding, c'est une look-up table).

Cette matrice contient donc $n_"dimensions" * n_"mots"$ param√®tres.

Context size : le nombre de tokens, chunks, que peut prendre le transformer.

![[image-152.png|618x342]]

Une **direction** dans l'espace correspond √† un genre, √† la pluralit√© du mot, la taille par exemple, etc.  On parle bien de **direction** et non pas de **dimension**, il n'y a pas une dimension li√©e √† chaque crit√®re !

> [!danger] aux vecteurs d'embeddings sont ajout√©s des informations √† propos de la position des √©l√©ments dans la phrase.
> 
> en fait on a des embeddings pour chaque position et on fait embedding mot + embedding position. De m√™me que pour les crit√®res de genre, pluralit√©, etc., on a une direction pour la position du mot dans la phrase.

> [!tip] Exemple de positional embedding
> 
> ```python
> def get_positional_embeddings(sequence_length, d):
>  result = torch.zeros(sequence_length, d)
>  for i in range(sequence_length):
>     for j in range(d // 2):
>         angle = i / (10000 ** (2 * j / d))
>         result[i, 2 * j] = torch.sin(torch.tensor(angle))
>         result[i, 2 * j + 1] = torch.cos(torch.tensor(angle))
> return result
>```
>
>`sequence_length` c'est le nombre de tokens et `d` c'est le nombre de dimensions. Pour chaque embedding, on remplit les dimensions paires avec 

### √âtape 2 : phase d'attention, partage de contexte entre les diff√©rents tokens

On veut une fa√ßon d'ajuster l'embedding des mots en fonction des autres mots √† c√¥t√©. Par exemple il y a sans doute une position dans l'espace qui correspond exactement √† "tour eiffel", et "tour" est mal positionn√© au d√©but.

**Q**ueries : les questions que le token se pose. pour la calculer, on a une matrice $W_Q$ qu'on multiplie √† l'embedding. 
**K**eys : une sorte de FAQ propos√©e par le token. de m√™me, on multiplie $W_K$ avec l'embedding.
**V**alues : les r√©ponses √† chaque question mise dans la FAQ (parce que les r√©ponses ne sont pas forc√©ment proches de la question !)

le nombre de dimensions de $W_K$ et $W_Q$ est bien plus petit que celui le nombre de dimensions des embeddings.

![[image-153.png|589x345]]

On va faire le dot product entre la query de chaque embedding et les keys des autres mots, et si on a un grand nombre alors ces mots sont sans doute int√©ressant.

On va donc mapper tous ces dot-products alors entre 0 et infinity entre 0 et 1 avec une fonction de softmax (qui s'applique sur chaque colonne parce que la somme de chaque **colonne** doit faire 1), puis les mutiplier par leur $V$ pour obtenir une weighted sums des r√©ponses les plus int√©ressantes.
Ensuite, on ajoute ce vecteur $"softmax"(K^T Q) V$ √† notre embedding original.

![[image-154.png|671x394]]

> [!tip] masking
> 
> Quand on donne une s√©quence de tokens √† notre GPT, et qu'on lui demande de pr√©dire le suivant, en fait il va quand m√™me essayer de pr√©dire chaque token qu'on lui d√©j√† donn√© (ce qui fait que quand on l'entra√Æne en lui disant "une cr√©ature verte mange une X", il va essayer de pr√©dire "une cr√©ature verte X"). pour √ßa, on veut √©viter que le mod√®le ait acc√®s aux mots suivants "une cr√©ature verte" pour √©viter qu'on lui spoile la r√©ponse.
> 
> C'est pour √ßa qu'on "masque" les mots suivants, en modifiant, avant d'appliquer softmax sur chaque colonne, le r√©sultat de leur dot product et en le mettant √† -infinity (comme √ßa √ßa donnera 0).

> [!tip] cross attention
> 
> Ce qu'on a vu jusque-l√† c'√©tait du self attention, le cross attention c'est quand on a deux types de donn√©es (par exemple une phrase source en anglais et une traduction en cours en fran√ßais, ou alors un speech audio, et une transcription en cours). Les queries viennent donc de la transcription, traduction en fran√ßais, et les keys/values viennent de l'audio ou de la phrase source.

> [!tip] multi-head attention
> 
> GPT 3 utilise 96 sets de queries, keys and values entra√Æn√©s pour d√©tecter et pr√™ter attention de fa√ßon diff√©rentes aux mots autour. Chacun de ces heads proposent une update de l'embedding de base $Delta E$, et pour calculer l'embedding final on additionne tous ces changements.
### √âtape 3 : multi layer perceptron

Dans cette √©tape, les vecteurs ne se parlent pas entre eux. On peut donc faire cette √©tape en parall√®le pour chaque embedding.

- on multiplie notre embedding avec une matrice qui contient des param√®tres (√©tape **lin√©aire**). on ajoute aussi un bias, typiquement utile pour que le r√©sultat soit positif si la r√©ponse est oui et n√©gatif sinon. Chaque ligne pose une question sur l'embedding par exemple "est-ce que c'est du code ?", "est-ce que c'est de l'anglais ?", etc. 
- le probl√®me c'est que c'est lin√©aire pour le moment, nous on veut une r√©ponse "oui" ou "non". on applique donc la fonction ReLU (Rectified Linear Unit).
- ensuite, on reconstruit l'embedding en ajoutant ou non des informations en fonction de si le neurone est actif

![[image-156.png]]

par exemple ici, si le neurone $n_0$ est 1 (c'est celui qui a demand√© "est-ce que l'embedding parle de Michael Jordan"), alors on ajoute √† notre embedding $C_0$ qui contient les informations √† propos de basketball, des chicago bulls, du num√©ro de maillot de Michael Jordan, etc.

![[image-157.png|635x266]]
### √âtape y : on r√©cup√®re une liste de probabilit√©s pour chaque mot

On prend le dernier vecteur et on utilise un unembedding matrix + softmax pour g√©n√©rer les probabilit√©s pour les mots. 
La unembedding matrix est l'inverse de la embedding matrix ?

## Primary Component Analysis

Imagine qu'on ait une liste de pays avec diff√©rentes valeurs pour le PIB, les services sociaux, l'esp√©rence de vie, etc. On ne peut pas tout repr√©senter sur un unique graphe. On utilise PCA pour obtenir 1, 2, ou 3 axes qui nous permettent d'obtenir une visualisation qui s√©pare les pays selon des crit√®res fusionn√©s (par exemple, les pays "tr√®s bonne esp√©rance de vie et tr√®s bons services sociaux" et les autres). Chaque axe suit un ordre d'importance (on fait une premi√®re s√©paration avec le premier axe, puis on divise en sous-groupe avec les autres axes), et est d√©correl√© des autres.

> [!tip] Intuitivement: la matrice de covariance
> 
> - "Quand le PIB augmente, est-ce que l‚Äôesp√©rance de vie augmente aussi ?" ‚Üí Covariance positive
> - "Quand un augmente, l‚Äôautre baisse ?" ‚Üí Covariance n√©gative
> - "Pas de lien ?" ‚Üí Covariance proche de 0

> [!question] Pourquoi la matrice de covariance est utile ?
> 
> Pour cela, on veut arriver √† savoir quelles variables varient ensemble, pour les rattacher √† un m√™me axe.
> 
> Au d√©but, on veut donc prendre l'axe qui nous fait perdre le moins d'information. Quand on projete un point, $x$ sur un vecteur $\vec{u}$ on obtient un nouveau vecteur $x'= (\vec{x}^T {u})\vec{u}$.
> 
> On peut mesurer l'information pr√©serv√©e avec $(x^T u )^2$.
> Au d√©but, on veut donc prendre le vecteur qui maximise l'information pr√©serv√©e.
> $$max "information" = max 1/n sum_i (x_i^T u)^2 = max 1/n sum_i (x_i^T u)(x_i^T u) $$
> $$ = max 1/n u^T sum_i x_i x_i^T u = max u^T C u$$
> o√π $C = 1/n sum_i x_i x_i^T$ est la matrice de covariance.
> 
> Cela revient √† r√©soudre le probl√®me (car $u^T u - 1 = 0$):
> $$ max u^T C u - lambda (u^T u  - 1) $$
> puis en diff√©renciant :
> $$ 2 C u - 2 lambda u = 0 <=> C u  = - lambda u$$
> donc le vecteur $u$ qui pr√©serve le plus d'information est un vecteur propre de la matrice de covariance !
> 
> Et on voit que la quantit√© d'information pr√©serv√©e ($u^T C u$) est √©gale √† $lambda$! donc on va prendre le vecteur avec la plus grande valeur propre. Une fois qu'on a trouv√© le vecteur propre (par exemple $u = [0.5, 0.6, 0.4]$), √ßa nous donne les poids de chaque colonne pour le graphe.

> [!question] Quelles √©tapes ?
> 
> 1. **Centrer les donn√©es**
>    
> On centre chaque colonne, on soustrait la moyenne de chaque variable √† toutes les valeurs de cette variable. On obtient une matrice centr√©e de taille $n times d$.
> 
> 2. Calcul de la **matrice de covariance**
>  
> $$"Cov"(X)= 1/(n ‚àí 1) X^T_"centered" ‚ÄãX_"centered"$$
> 
> R√©sultat : une matrice $d times d$ (une case pour chaque paire de variables).
> 
> 3. On calcule les vecteurs propres de la matrice.
>    
>    Chaque vecteur propre pointe vers une **direction importante** dans l‚Äôespace des variables (ex. une direction combinant PIB + esp√©rance de vie).
>    Les valeurs propres disent **quelle proportion de la variance totale** est expliqu√©e par chaque axe.



