### 2019

1. **parmi ces quatre affirmations, lesquelles sont exactes** pour l’algorithme SVM ?
	1. La fonction duale lagrangienne fournit une borne supérieure au problème d’origine

2. **You find that your linear regression model is underfitting the data. In such a situation, which of the following options can potentially help?**
	1. Using polynomial feature expansion $arrow$ **vrai**, permet de mieux fit les données

3. **Which of the following statements are true for a k-NN classifier?**
	1. k-NN can be used only in classification but not for regression problems $arrow$ **faux**, on peut faire la moyenne des voisins par exemple
	2. The decision boundary becomes smoother as we increase the value of k $arrow$ **vrai**, on fait 

### 2022

1. We have the same setup as in the previous question. Which of the following methods are able to fit the dataset perfectly?
	1. ernel regression with kernel k(xi, xj ) = (x(1)i )2(x(1)j )2 + (x(2)i )2(x(2)j )2
	2. This kernel corresponds to the explicit mapping ϕ(x)=(x(1) 2,x(2) 2)\phi(\mathbf x)=\big(x^{(1)\,2},x^{(2)\,2}\big)ϕ(x)=(x(1)2,x(2)2). In that new space the quantity r2=x(1) 2+x(2) 2r^2 = x^{(1)\,2}+x^{(2)\,2}r2=x(1)2+x(2)2 is _linear_, so a single hyperplane w1x(1) 2+w2x(2) 2+b=0w_1x^{(1)\,2}+w_2x^{(2)\,2}+b=0w1​x(1)2+w2​x(2)2+b=0 (choose w1=w2>0w_1=w_2>0w1​=w2​>0) becomes the circle that cleanly separates inner from outer ring.
